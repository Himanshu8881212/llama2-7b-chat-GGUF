# llama2-7b-chat-GGUF
Inferencing of llama2-7b-chat GGUF quantized model by the bloke with conversational buffer memory using CLI UI

Setting Up and Running the Model Offline with CLI UI on the CPU

Before starting, please note:
This setup assumes you are operating within a virtual environment. Ensure you're referencing the correct directory paths inside this environment when following the steps below.

1. Downloading the Model:
- Download the model using the `automodelforcausallm` function or fetch it directly from the Hugging Face model hub.

2. Updating Model Path:
- After downloading, open the code.
- Find and update the model path to the location where you saved the model inside your virtual environment.

3. Editing the Notepad Bash Scripting File:
- Find the bash scripting file provided. This might have an extension like `.txt` or similar.
- Open it in Notepad or a similar text editor.
- Make the necessary modifications, and ensure reference of the correct directory paths due to the virtual environment.

4. Saving and Executing the Batch File:
- After editing, save the file with a `.bat` extension.
- Run the file either by double-clicking or executing it via command prompt.

5. Using the Offline Model with CLI UI:
- Once you've executed the `.bat` file, the offline model should be operational.
- Interact using the provided Command Line Interface (CLI) for a user-friendly experience.

When faced with challenges, refer back to this guide.
